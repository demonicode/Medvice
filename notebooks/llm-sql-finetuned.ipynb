{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6362a562-f3ce-4dce-9678-ce317e554a04",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ce195c-cd70-4de6-bd72-c8889fb697f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install intel_extension_for_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9236e9d1-e75e-4089-9a4d-27421521cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import site\n",
    "# from pathlib import Path\n",
    "\n",
    "# # !echo \"Installation in progress, please wait...\"\n",
    "# # !{sys.executable} -m pip cache purge > /dev/null\n",
    "# # !{sys.executable} -m pip install peft==0.5.0 --no-deps > /dev/null\n",
    "# # !{sys.executable} -m pip install accelerate==0.23.0 --no-deps --no-warn-script-location > /dev/null\n",
    "# # !{sys.executable} -m pip install  transformers==4.34.0 --no-warn-script-location > /dev/null \n",
    "# # !{sys.executable} -m pip install datasets==2.15 --no-warn-script-location > /dev/null 2>&1 \n",
    "# # !{sys.executable} -m pip install fsspec==2023.9.2 > /dev/null  2>&1\n",
    "\n",
    "# # removed\n",
    "# #!{sys.executable} -m pip install --pre bigdl-llm[xpu]==2.4.0b20231116 --no-warn-script-location -f https://developer.intel.com/ipex-whl-stable-xpu > /dev/null\n",
    "\n",
    "\n",
    "# # added\n",
    "# # !{sys.executable} -m pip install bigdl_core_xe\n",
    "# !pip install BigDL\n",
    "# !pip install BigDL-llm\n",
    "# # !pip install bigdl_core_xe\n",
    "\n",
    "# !echo \"Installation completed.\"\n",
    "\n",
    "# def get_python_version():\n",
    "#     return \"python\" + \".\".join(map(str, sys.version_info[:2]))\n",
    "\n",
    "# def set_local_bin_path():\n",
    "#     local_bin = str(Path.home() / \".local\" / \"bin\") \n",
    "#     local_site_packages = str(\n",
    "#         Path.home() / \".local\" / \"lib\" / get_python_version() / \"site-packages\"\n",
    "#     )\n",
    "#     sys.path.append(local_bin)\n",
    "#     sys.path.insert(0, site.getusersitepackages())\n",
    "#     sys.path.insert(0, sys.path.pop(sys.path.index(local_site_packages)))\n",
    "\n",
    "# set_local_bin_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31eb9cf2-abcf-48f8-918b-37a18f85ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"intel_extension_for_pytorch\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"torchvision.io.image\", lineno=13\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using the default legacy behaviour\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Parameter.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"This implementation of AdamW is deprecated\",\n",
    ")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"28\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bigdl\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.qlora import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training as prepare_model,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    \n",
    ")\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18326c-bd74-40e8-b740-7566a6f9c0ef",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ecd7df-b7ce-48b0-ba9f-04b7ec0c1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeab82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(base_model_id: str, model_cache_path):\n",
    "    \"\"\"Downloads / Loads the pre-trained model in NF4 datatype and tokenizer based on the given base model ID for training.\"\"\"\n",
    "    local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "    local_model_path = os.path.join(model_cache_path, local_model_id)\n",
    "    print(f\"local model path is: {local_model_path}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "        logging.info(\n",
    "            f\"Model not found locally. Downloading {base_model_id} to cache...\"\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Tokenizer not found locally. Downloading tokenizer for {base_model_id} to cache...\"\n",
    "        )\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777656b-74f4-4ebc-b741-a5d50bc6e79a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**FineTuner**\n",
    "\n",
    "The `FineTuner` class encapsulates the entire process of fine-tuning llms for tasks such as text-to-SQL conversion.\n",
    "\n",
    "\n",
    "**Tokenization Strategy**\n",
    "\n",
    "The tokenization process is tailored to the type of model being fine-tuned. For instance, if we are working with a Llama model, we utilize a `LlamaTokenizer` to ensure compatibility with the model's expected input format. For other models, a generic `AutoTokenizer` is used. We configure the tokenizer to pad from the left side (`padding_side=\"left\"`) and set the pad token ID to 0.\n",
    "\n",
    "**Data Tokenization and Preparation**\n",
    "\n",
    "The `tokenize_data` method is where the fine-tuner ingests raw text data and converts it into a format suitable for training the model. This method handles the addition of end-of-sequence tokens, truncation to a specified `cutoff_len`, and conditioning on the input for training.\n",
    "\n",
    "**Dataset Handling**\n",
    "\n",
    "`prepare_data` manages the splitting of data into training and validation sets, applying the `tokenize_data` transformation to each entry. This ensures that our datasets are ready for input into the model, with all necessary tokenization applied.\n",
    "\n",
    "**Training Process**\n",
    "\n",
    "Finally, the `train_model` method orchestrates the training process, setting up the `Trainer` with the correct datasets, training arguments, and data collator. The fine-tuning process is encapsulated within the `finetune` method, which strings together all the previous steps into a coherent pipeline, from model setup to training execution.\n",
    "\n",
    "**Using QLoRA for Efficient Fine-Tuning**\n",
    "1. Load a pretrained model (e.g., LLaMA) in low precision with `load_in_low_bit=\"nf4\"` for 4-bit quantized weights.\n",
    "2. Prepare the quantized model with `prepare_model(model)`, handling weight quantization.\n",
    "3. Add LoRA adapters via `get_peft_model(model, config)` for setting adapter parameters.\n",
    "4. Fine-tune with `Trainer`, focusing gradients on adapters while keeping base model weights fixed.\n",
    "\n",
    "**Code Implementation**\n",
    "- Model loading with BigDL's `AutoModelForCausalLM`, initializing in 4-bit using `load_in_low_bit=\"nf4\"`.\n",
    "- `prepare_model()` quantizes the model weights.\n",
    "- `get_peft_model()` adds LoRA adapters.\n",
    "- Trainer handles fine-tuning, optimizing only adapter weights.\n",
    "\n",
    "\n",
    "So in summary, we leverage QLoRA in BigDL to load the base LLM in low precision, inject adapters with `peft`, and efficiently finetune by optimizing just the adapters end-to-end while keeping the base model fixed. This unlocks huge memory savings, allowing us to adapt giant models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb983d62-7243-4aee-9f17-a5846f050fbd",
   "metadata": {},
   "source": [
    "## Fine-Tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8f4cb8-0da5-4572-bd07-bdae1db897a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    \"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id: str, model_path: str, device: torch.device, model_cache_path):\n",
    "        \"\"\"\n",
    "        Initialize the FineTuner with base model, model path, and device.\n",
    "\n",
    "        Parameters:\n",
    "            base_model_id (str): Id of pre-trained model to use for fine-tuning.\n",
    "            model_path (str): Path to save the fine-tuned model.\n",
    "            device (torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id, model_cache_path)\n",
    "\n",
    "\n",
    "    def tokenize_data(\n",
    "        self, data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes dataset of SQL related data points consisting of questions, context, and answers.\n",
    "\n",
    "        Parameters:\n",
    "            data_points (dict): A batch from the dataset containing 'question', 'context', and 'answer'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question = data_points[\"question\"]\n",
    "            context = data_points[\"context\"]\n",
    "            answer = data_points[\"answer\"]\n",
    "            if train_on_inputs:\n",
    "                user_prompt = generate_prompt_sql(question, context)\n",
    "                tokenized_user_prompt = self.tokenizer(\n",
    "                    user_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=cutoff_len,\n",
    "                    padding=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "                if add_eos_token:\n",
    "                    user_prompt_len -= 1\n",
    "\n",
    "            combined_text = generate_prompt_sql(question, context, answer)\n",
    "            tokenized = self.tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            if (\n",
    "                tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "            ):\n",
    "                tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            if train_on_inputs:\n",
    "                tokenized[\"labels\"] = [-100] * user_prompt_len + tokenized[\"labels\"][\n",
    "                    user_prompt_len:\n",
    "                ]\n",
    "\n",
    "            return tokenized\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in batch tokenization: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def prepare_data(self, data, val_set_size=100) -> Dataset:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        try:\n",
    "            train_val_split = data[\"train\"].train_test_split(\n",
    "                test_size=val_set_size, shuffle=True, seed=42\n",
    "            )\n",
    "            train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "            val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "            return train_data, val_data\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in preparing data: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, train_data, val_data, training_args, lora_config):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with the given training and validation data.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (Dataset): Training data.\n",
    "            val_data (Optional[Dataset]): Validation data.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model = prepare_model(self.model)\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    self.tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "            self.model.config.use_cache = False\n",
    "            trainer.train()\n",
    "            self.model.save_pretrained(self.model_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model training: {e}\")\n",
    "\n",
    "    def finetune(self, data_path, training_args, lora_config):\n",
    "        \"\"\"\n",
    "        Execute the fine-tuning pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): Path to the data for fine-tuning.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = load_dataset(data_path)\n",
    "            train_data, val_data = self.prepare_data(data)\n",
    "            self.train_model(train_data, val_data, training_args, lora_config)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupt received, saving model...\")\n",
    "            self.model.save_pretrained(f\"{self.model_path}_interrupted\")\n",
    "            print(f\"Model saved to {self.model_path}_interrupted\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fintuning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a39122-eafc-483c-ad81-9c55de15e936",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENABLE_WANDB = False\n",
    "\n",
    "def lets_finetune(\n",
    "    device,#=DEVICE,\n",
    "    model,#=BASE_MODEL,\n",
    "    MODEL_PATH,# = MODEL_PATH\n",
    "    model_cache_path, # not here before\n",
    "    data_path,\n",
    "    lora_config,\n",
    "    per_device_batch_size=4,\n",
    "    warmup_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=100,\n",
    "    gradient_accum_steps=4,\n",
    "    \n",
    "    \n",
    "):\n",
    "    try:\n",
    "        # Training parameters\n",
    "        save_steps = 20\n",
    "        eval_steps = 20\n",
    "        max_grad_norm = 0.3\n",
    "        save_total_limit = 3\n",
    "        logging_steps = 20\n",
    "\n",
    "        # Initialize the finetuner with the model and device information\n",
    "        finetuner = FineTuner(\n",
    "            base_model_id=model, model_path=MODEL_PATH, device=device, model_cache_path = model_cache_path\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            save_steps=save_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            #max_grad_norm=max_grad_norm,\n",
    "            bf16=True,\n",
    "            #lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length=True,\n",
    "            save_total_limit=save_total_limit,\n",
    "            logging_steps=logging_steps,\n",
    "            optim=\"adamw_hf\",\n",
    "            output_dir=f\"./lora_adapters/{model}\",\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "        )\n",
    "\n",
    "        # Start fine-tuning\n",
    "        finetuner.finetune(data_path, training_args, lora_config)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f27d6-0771-41c6-aa18-1b02d4ae178b",
   "metadata": {},
   "source": [
    "## Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a0e504-46b7-4c6e-8d80-b28798607ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "INFERENCE_DEVICE = torch.device(\"cpu\")  # change this to `xpu` to use Intel GPU for inference  \n",
    "\n",
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(base_model_id: str, model_cache_path):\n",
    "    \"\"\"Downloads / Load the pre-trained model in 4bit and tokenizer based on the given base model ID for inference.\"\"\"\n",
    "    local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "    local_model_path = os.path.join(model_cache_path, local_model_id)\n",
    "    print(f\"local model path is: {local_model_path}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_4bit=True,\n",
    "            optimize_model=True,\n",
    "            use_cache=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Model not found locally. Downloading {base_model_id} to cache...\"\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_4bit=True,\n",
    "            optimize_model=True,\n",
    "            use_cache=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Tokenizer not found locally. Downloading tokenizer for {base_model_id} to cache...\"\n",
    "        )\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer\n",
    "\n",
    "class TextToSQLGenerator:\n",
    "    \"\"\"Handles SQL query generation for a given text prompt.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_model_id, model_cache_path,#=BASE_MODEL, \n",
    "        use_adapter=False, lora_checkpoint=None, loaded_base_model=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the InferenceModel class.\n",
    "        Parameters:\n",
    "            use_adapter (bool, optional): Whether to use LoRA model. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if loaded_base_model:\n",
    "                self.model = loaded_base_model.model\n",
    "                self.tokenizer = loaded_base_model.tokenizer\n",
    "            else:\n",
    "                self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id, model_cache_path=model_cache_path)\n",
    "            if use_adapter:\n",
    "                self.model = PeftModel.from_pretrained(self.model, lora_checkpoint)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during model initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.model.to(INFERENCE_DEVICE)\n",
    "        self.max_length = 512\n",
    "\n",
    "\n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generates an SQL query based on the given prompt.\n",
    "        Parameters:\n",
    "            prompt (str): The SQL prompt.\n",
    "        Returns:\n",
    "            str: The generated SQL query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_prompt = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.to(INFERENCE_DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.xpu.amp.autocast():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=encoded_prompt,\n",
    "                        do_sample=True,\n",
    "                        max_length=self.max_length,\n",
    "                        temperature=0.3,\n",
    "                        repetition_penalty=1.2,\n",
    "                    )\n",
    "            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during query generation: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8417146-0c28-4996-81a4-c4b0857d81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "INFERENCE_DEVICE = torch.device(\"cpu\")  \n",
    "\n",
    "\n",
    "# let's use some fake sample data\n",
    "samples = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"question\": \"What is the capacity of the stadium where the team 'Mountain Eagles' plays?\",\n",
    "    \"context\": \"CREATE TABLE stadium_info (team_name VARCHAR, stadium_name VARCHAR, capacity INT)\",\n",
    "    \"target\": \"SELECT capacity FROM stadium_info WHERE team_name = 'Mountain Eagles';\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How many goals did player John Smith score last season?\",\n",
    "    \"context\": \"CREATE TABLE player_stats (player_name VARCHAR, goals_scored INT, season VARCHAR)\",\n",
    "    \"target\": \"SELECT SUM(goals_scored) FROM player_stats WHERE player_name = 'John Smith' AND season IN ('last season');\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the operating hours for the Central Library on weekends?\",\n",
    "    \"context\": \"CREATE TABLE library_hours (library_name VARCHAR, day_of_week VARCHAR, open_time TIME, close_time TIME)\",\n",
    "    \"target\": \"SELECT * FROM library_hours WHERE library_name = 'Central' AND day_of_week IN ('Saturday', 'Sunday');\"\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def _extract_sections(output):\n",
    "    input_section = output.split(\"### Input:\")[1].split(\"### Context:\")[0]\n",
    "    context_section = output.split(\"### Context:\")[1].split(\"### Response:\")[0]\n",
    "    response_section = output.split(\"### Response:\")[1]\n",
    "    return input_section, context_section, response_section\n",
    "\n",
    "def run_inference(sample_data, model, finetuned=False):\n",
    "    if INFERENCE_DEVICE.type.startswith(\"xpu\"):\n",
    "        torch.xpu.empty_cache()\n",
    "    \n",
    "    color = \"#4CAF52\" if finetuned else \"#2196F4\"\n",
    "    model_type = \"finetuned\" if finetuned else \"base\"\n",
    "    display(HTML(f\"<div style='color:{color};'>Processing queries on {INFERENCE_DEVICE} please wait...</div>\"))\n",
    "\n",
    "    \n",
    "    for index, row in enumerate(sample_data):\n",
    "        try:\n",
    "            prompt = generate_prompt_sql(row[\"question\"], context=row[\"context\"])\n",
    "            output = model.generate(prompt)            \n",
    "            input_section, context_section, response_section = _extract_sections(output)\n",
    "            \n",
    "            tabbed_output = f\"\"\"\n",
    "            <details>\n",
    "                <summary style='color: {color};'><b>{model_type} model - Sample {index+1}</b> (Click to expand)</summary>\n",
    "                <div style='padding-left: 20px;'>\n",
    "                    <p><b>Expected input 📝:</b><br>{input_section}</p>\n",
    "                    <p><b>Expected context 📚:</b><br>{context_section}</p>\n",
    "                    <p><b>Generated response 💡:</b><br>{response_section}</p>\n",
    "                    <p><b>Target response 🎯:</b><br>{row[\"target\"]}</p>\n",
    "                    <p><b>Correct?:</b><br>{row[\"target\"] in response_section}</p>\n",
    "                </div>\n",
    "            </details>\n",
    "            <hr style='border-top: 1px solid #bbb;'>\"\"\"  # Subtle separator\n",
    "            display(HTML(tabbed_output))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Exception occurred during sample processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dfeb0-6da2-4338-bea7-be605a235155",
   "metadata": {},
   "source": [
    "## Generate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2c6f25-5c45-4061-9b62-91233149b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model):\n",
    "    print(\"Model:\",model)\n",
    "    model = model.replace(\"/\", \"--\")\n",
    "    MODEL_PATH = f\"./final_model/{model}\"\n",
    "    BASE_MODEL = model\n",
    "    #DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "    device = torch.device(\"xpu\")\n",
    "    data_path = \"b-mc2/sql-create-context\"\n",
    "    \n",
    "    LORA_CONFIG = LoraConfig(\n",
    "        r=16,  # rank\n",
    "        lora_alpha=32,  # scaling factor\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "    \n",
    "    lets_finetune(device = device, model = BASE_MODEL, MODEL_PATH = MODEL_PATH, model_cache_path = MODEL_CACHE_PATH, lora_config = LORA_CONFIG, data_path=data_path)\n",
    "    \n",
    "    base_model = TextToSQLGenerator(\n",
    "        base_model_id = model,\n",
    "        model_cache_path = MODEL_CACHE_PATH,\n",
    "        use_adapter=False,\n",
    "        lora_checkpoint=\"\",\n",
    "    )\n",
    "    \n",
    "    USING_CHECKPOINT=100\n",
    "    LORA_CHECKPOINT = f\"./lora_adapters/{BASE_MODEL}/checkpoint-{USING_CHECKPOINT}/\"\n",
    "    if os.path.exists(LORA_CHECKPOINT):\n",
    "        sample_data = json.loads(samples)\n",
    "        run_inference(sample_data, model=base_model)\n",
    "        finetuned_model = TextToSQLGenerator(\n",
    "            use_adapter=True,\n",
    "            lora_checkpoint=LORA_CHECKPOINT,\n",
    "            loaded_base_model=base_model,\n",
    "            base_model_id = BASE_MODEL,\n",
    "            model_cache_path = MODEL_CACHE_PATH\n",
    "        )\n",
    "        run_inference(sample_data, model=finetuned_model, finetuned=True)\n",
    "        return finetuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57bb2158-f72c-4f07-b0af-ce6e0027420e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NousResearch/CodeLlama-7b-hf\n",
      "local model path is: /home/common/data/Big_Data/GenAI/llm_models/NousResearch--CodeLlama-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342f39c38e7c4af7bb28c14470094555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab138ef186894db3acdf02622fdc3976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521a0e1752c54256aafc5a5480410cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3162, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'eval_loss': 2.4766347408294678, 'eval_runtime': 4.8428, 'eval_samples_per_second': 20.649, 'eval_steps_per_second': 2.684, 'epoch': 0.0}\n",
      "{'loss': 2.2985, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.01}\n",
      "{'eval_loss': 2.2410101890563965, 'eval_runtime': 4.8391, 'eval_samples_per_second': 20.665, 'eval_steps_per_second': 2.686, 'epoch': 0.01}\n",
      "{'loss': 1.9465, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
      "{'eval_loss': 1.9794702529907227, 'eval_runtime': 4.8439, 'eval_samples_per_second': 20.644, 'eval_steps_per_second': 2.684, 'epoch': 0.01}\n",
      "{'loss': 1.7401, 'learning_rate': 5e-06, 'epoch': 0.02}\n",
      "{'eval_loss': 1.8097870349884033, 'eval_runtime': 4.8516, 'eval_samples_per_second': 20.612, 'eval_steps_per_second': 2.68, 'epoch': 0.02}\n",
      "{'loss': 1.5892, 'learning_rate': 0.0, 'epoch': 0.02}\n",
      "{'eval_loss': 1.7447198629379272, 'eval_runtime': 4.834, 'eval_samples_per_second': 20.687, 'eval_steps_per_second': 2.689, 'epoch': 0.02}\n",
      "{'train_runtime': 567.0342, 'train_samples_per_second': 2.822, 'train_steps_per_second': 0.176, 'train_loss': 1.978087158203125, 'epoch': 0.02}\n",
      "local model path is: /home/common/data/Big_Data/GenAI/llm_models/NousResearch--CodeLlama-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c905fbf925ef4472bfca8bf28f63b256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='color:#2196F4;'>Processing queries on cpu please wait...</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #2196F4;'><b>base model - Sample 1</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "What is the capacity of the stadium where the team 'Mountain Eagles' plays?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE stadium_info (team_name VARCHAR, stadium_name VARCHAR, capacity INT)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT capacity FROM stadium_info WHERE team_name = 'Mountain Eagles';\n",
       "\n",
       "### Example:\n",
       "Input: What was the average score for each game played by player 'Billie Jean King'?\n",
       "Context: CREATE TABLE tennis_player_stats (player_id VARCHAR, match_date DATE, opponent VARCHAR, outcome ENUM('win', 'loss'), score FLOAT)\n",
       "Response: SELECT AVG(score) FROM tennis_player_stats WHERE player_id='Billie Jean King';\n",
       "\n",
       "### Challenge:\n",
       "Write an algorithm which will take in a table name and column names as input, then return all possible queries based on the information provided. For example, if you have a table called \"users\" with columns id, firstName, lastName, emailAddress, passwordHash, and age, your program should be able to produce these results from this data set:\n",
       "\n",
       "* Who is the oldest user?\n",
       "* How many users do we have?\n",
       "* Do any of our users live in New York City?\n",
       "* Are there two users named Bill Gates?\n",
       "* Is anyone under 20 years old?\n",
       "* Does anybody live at address '1600 Pennsylvania Ave NW'?\n",
       "* Which users were born before 1985?\n",
       "* How many users live in New Jersey?\n",
       "* How many users live in California?\n",
       "* What percentage of users are male?\n",
       "* What percentage of users are female?\n",
       "* What percentage of users are over 30 years old?\n",
       "* What percentage of users are under 30 years old?\n",
       "* What percentage of users are between 40 and 49 years old?\n",
       "* What percentage of users are between 50 and 59 years old?\n",
       "* What percentage of users are over 60 years old?\n",
       "* What percentage of users are under 6</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT capacity FROM stadium_info WHERE team_name = 'Mountain Eagles';</p>\n",
       "                    <p><b>Correct?:</b><br>True</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #2196F4;'><b>base model - Sample 2</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "How many goals did player John Smith score last season?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE player_stats (player_name VARCHAR, goals_scored INT, season VARCHAR)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT SUM(goals_scored) FROM player_stats WHERE player_name = 'John Smith' AND season='2019';\n",
       "\n",
       "---\n",
       "\n",
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "What was the average number of goals scored by players who were born in January 1985?\n",
       "\n",
       "### Context:\n",
       "CREATE TABLE player_stats (player_id INT PRIMARY KEY AUTO_INCREMENT NOT NULL, player_name VARCHAR, birthdate DATE, goals_scored INT);\n",
       "INSERT INTO player_stats VALUES ('Jane Doe', '1985-01-31');\n",
       "INSERT INTO player_stats VALUES ('John Smith', '1984-06-07');\n",
       "INSERT INTO player_stats VALUES ('Mary Jane', '1985-01-15');\n",
       "INSERT INTO player_stats VALUES ('Bob Bobberson', '1985-01-15');\n",
       "\n",
       "</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT SUM(goals_scored) FROM player_stats WHERE player_name = 'John Smith' AND season IN ('last season');</p>\n",
       "                    <p><b>Correct?:</b><br>False</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #2196F4;'><b>base model - Sample 3</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "What are the operating hours for the Central Library on weekends?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE library_hours (library_name VARCHAR, day_of_week VARCHAR, open_time TIME, close_time TIME)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT * FROM library_hours WHERE DAYOFWEEK(open_time) IN (6,7);\n",
       "\n",
       "### Questions:\n",
       "1. What are the operating hours for the Central Library on weekends?\n",
       "2ubuntu@ip-10-0-54-89:~/workspace/sql$ python3 sqltext2query.py \"What are the operating hours for the Central Library on weekends?\"\n",
       "```python\n",
       "import re\n",
       "from nltk import word_tokenize\n",
       "from nltk.corpus import stopwords\n",
       "stop = set(stopwords.words('english'))\n",
       "def tokenizer(sentence):\n",
       "    tokens = [word for word in sentence if not word in stop]\n",
       "    return tokens\n",
       "\n",
       "class SqlTextToQuery():\n",
       "\tdef __init__(self):\n",
       "\t\tpass\n",
       "\tdef get_question(self, question):\n",
       "\t\treturn self._get_question(question).strip()\n",
       "\tdef _get_question(self, question):\n",
       "\t\tif '?' in question:\n",
       "\t\t\treturn question[:question.index(\"?\")]\n",
       "\t\telse:\n",
       "\t\t\traise Exception(\"No ? found\")\n",
       "\tdef get_table_names(self, table_context):\n",
       "\t\ttables = []\n",
       "\t\tfor line in table_context.split(\"\\n\"):\n",
       "\t\t\tline = line.replace(\",\", \"\")\n",
       "\t\t\tmatch = re.search('\\w+', line)\n",
       "\t\t\tif match:\n",
       "\t\t\t\ttables += [match[0]]\n",
       "\t\treturn list(set(tables)) # remove duplicates\n",
       "\tdef get_field_names(self, fields):\n",
       "\t\tfields = fields.lower().replace(\".\",\"\").replace(', Поле','')\n",
       "\t\tfields = re.sub(' +', ' ', fields)</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT * FROM library_hours WHERE library_name = 'Central' AND day_of_week IN ('Saturday', 'Sunday');</p>\n",
       "                    <p><b>Correct?:</b><br>False</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='color:#4CAF52;'>Processing queries on cpu please wait...</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #4CAF52;'><b>finetuned model - Sample 1</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "What is the capacity of the stadium where the team 'Mountain Eagles' plays?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE stadium_info (team_name VARCHAR, stadium_name VARCHAR, capacity INT)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT capacity FROM stadium_info WHERE team_name = \"Mountain Eagles\"</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT capacity FROM stadium_info WHERE team_name = 'Mountain Eagles';</p>\n",
       "                    <p><b>Correct?:</b><br>False</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #4CAF52;'><b>finetuned model - Sample 2</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "How many goals did player John Smith score last season?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE player_stats (player_name VARCHAR, goals_scored INT, season VARCHAR)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT COUNT(*) FROM player_stats WHERE player_name = 'John Smith' AND season='2019/</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT SUM(goals_scored) FROM player_stats WHERE player_name = 'John Smith' AND season IN ('last season');</p>\n",
       "                    <p><b>Correct?:</b><br>False</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "                <summary style='color: #4CAF52;'><b>finetuned model - Sample 3</b> (Click to expand)</summary>\n",
       "                <div style='padding-left: 20px;'>\n",
       "                    <p><b>Expected input 📝:</b><br>\n",
       "What are the operating hours for the Central Library on weekends?\n",
       "\n",
       "</p>\n",
       "                    <p><b>Expected context 📚:</b><br>\n",
       "CREATE TABLE library_hours (library_name VARCHAR, day_of_week VARCHAR, open_time TIME, close_time TIME)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Generated response 💡:</b><br>\n",
       "SELECT * FROM library_hours WHERE day_of_week = 'Saturday' OR day_of_week = 'Sunday';\n",
       "\n",
       "### Input:\n",
       "How many books does the Central Library have in its collection?\n",
       "\n",
       "### Context:\n",
       "CREATE TABLE book (title VARCHAR, year INT, author_id INT, num_pages INT, genre VARCHAR, publisher VARCHAR, price DOUBLE)\n",
       "\n",
       "</p>\n",
       "                    <p><b>Target response 🎯:</b><br>SELECT * FROM library_hours WHERE library_name = 'Central' AND day_of_week IN ('Saturday', 'Sunday');</p>\n",
       "                    <p><b>Correct?:</b><br>False</p>\n",
       "                </div>\n",
       "            </details>\n",
       "            <hr style='border-top: 1px solid #bbb;'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODELS = {\n",
    "    \"0\": \"NousResearch/Nous-Hermes-Llama-2-7b\",  # https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\n",
    "    \"1\": \"NousResearch/Llama-2-7b-chat-hf\",  # https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
    "    \"2\": \"NousResearch/Llama-2-13b-hf\",  # https://huggingface.co/NousResearch/Llama-2-13b-hf\n",
    "    \"3\": \"NousResearch/CodeLlama-7b-hf\",  # https://huggingface.co/NousResearch/CodeLlama-7b-hf\n",
    "    \"4\": \"Phind/Phind-CodeLlama-34B-v2\",  # https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\n",
    "    \"5\": \"openlm-research/open_llama_3b_v2\",  # https://huggingface.co/openlm-research/open_llama_3b_v2\n",
    "    \"6\": \"openlm-research/open_llama_13b\",  # https://huggingface.co/openlm-research/open_llama_13b\n",
    "    \"7\": \"HuggingFaceH4/zephyr-7b-beta\", # https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
    "}\n",
    "# data_path = \"b-mc2/sql-create-context\"\n",
    "# data = load_dataset(data_path)\n",
    "\n",
    "# def prepare_data(data, val_set_size=100) -> Dataset:\n",
    "#     # \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "#     # train_data = train_val_split[\"train\"]\n",
    "#     # val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "#     seed = 0\n",
    "#     temp_a = data['train'].train_test_split(test_size=0.1, seed=seed)\n",
    "#     temp_b = temp_a['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "#     return DatasetDict({\n",
    "#         'train': temp_a['train'].shuffle().map(tokenize_data)],\n",
    "#         'test': temp_b['test'],\n",
    "#         'validation': temp_b['train']\n",
    "#     })\n",
    "\n",
    "    \n",
    "\n",
    "# prepared_ds = prepare_data(data)\n",
    "\n",
    "#for model_name in test_models:\n",
    "m = generate_model(BASE_MODELS[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659586a3-a698-4507-89ef-d636936f00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"juierror/text-to-sql-with-table-schema\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"juierror/text-to-sql-with-table-schema\")\n",
    "\n",
    "def prepare_input(question: str, table: List[str]):\n",
    "    table_prefix = \"table:\"\n",
    "    question_prefix = \"question:\"\n",
    "    join_table = \",\".join(table)\n",
    "    inputs = f\"{question_prefix} {question} {table_prefix} {join_table}\"\n",
    "    input_ids = tokenizer(inputs, max_length=700, return_tensors=\"pt\").input_ids\n",
    "    return input_ids\n",
    "\n",
    "def inference(question: str, table: List[str], model) -> str:\n",
    "    input_data = prepare_input(question=question, table=table)\n",
    "    input_data = input_data.to(model.device)\n",
    "    outputs = model.generate(inputs=input_data, num_beams=10, top_k=10, max_length=700)\n",
    "    result = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "#print(inference(question=\"get people name with age equal 25\", table=[\"id\", \"name\", \"age\"], model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf9b4e6-4903-439d-adc0-730bd6712d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ueda36fa477dc7ba9b01449685e6433d/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT capacity FROM table WHERE team_name = mountains eagles\n",
      "SELECT goals_scored FROM table WHERE player_name = john smith AND season = last\n",
      "SELECT day_of_week FROM table WHERE library_name = central library\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the capacity of the stadium where the team 'Mountain Eagles' plays?\"\n",
    "t = ['team_name', 'stadium_name', 'capacity']\n",
    "print(inference(question=q, table=t, model=model))\n",
    "\n",
    "q = \"How many goals did player John Smith score last season?\"\n",
    "t = ['player_name', 'goals_scored', 'season']\n",
    "print(inference(question=q, table=t, model=model))\n",
    "\n",
    "q = \"What are the operating hours for the Central Library on weekends?\"\n",
    "t = ['library_name', 'day_of_week', 'open_time', 'close_time']\n",
    "print(inference(question=q, table=t, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b490c02-468b-4e0f-8e96-0f5556740f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_data = json.loads(samples)\n",
    "# INFERENCE_DEVICE = torch.device(\"xpu\")\n",
    "# run_inference(sample_data, m, finetuned=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
